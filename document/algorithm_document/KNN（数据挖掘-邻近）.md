#KNN（k-NearestNeighbor 邻近算法）
核心思想：如果一个样本在特征空间中的k个最相邻的样本中的大多数属于某一个类别，则该样本也属于这个类别，并具有这个类别上样本的特性

例如：在一个二维平面内，存在三角与方块两个特征，先加入一个点，判断该点属于三角特征还是方块特征，以这个点为圆心，以一个较小树为圆心画圆，有两个三角，一个方块在内，说明三角较多，该点判断为三角特征，如果以较大半径为圆，有三个方块，两个三角，说明该点为方块特征

KNN主要用于分类和回归
	
KNN优点：
>1.简单，易于理解，易于实现，无需估计参数，无需训练；
>
>2.适合对稀有事件进行分类；
>
>3.特别适合于多分类问题(multi-modal,对象具有多个类别标签)， kNN比SVM的表现要好。

KNN缺点：
>1.易产生局部最优
>
>2.需要遍历范围内的特征点，计算量较大

KNN改进：
>1.分类效率：事先对样本属性进行约简，删除对分类结果影响较小的属性，快速的得出待分类样本的类别
>
>2.分类效果：采用权值的方法（和该样本距离小的邻居权值大）来改进
